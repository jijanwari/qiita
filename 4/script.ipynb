{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import timeit\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "\n",
    "class MyCallback(Callback):\n",
    "    def __init__(self, output_path=\".\"):\n",
    "        # Some algorithms compute multiple episodes at once since they are multi-threaded.\n",
    "        # We therefore use a dictionary that is indexed by the episode to separate episodes\n",
    "        # from each other.\n",
    "        self.episode_start = {}\n",
    "        self.observations = {}\n",
    "        self.rewards = {}\n",
    "        self.actions = {}\n",
    "        self.metrics = {}\n",
    "        self.step = 0\n",
    "        self.lastreward = -99999999\n",
    "        self.output_path = output_path\n",
    "\n",
    "    def on_train_begin(self, logs):\n",
    "        self.train_start = timeit.default_timer()\n",
    "        self.metrics_names = self.model.metrics_names\n",
    "        print('Training for {} steps ...'.format(self.params['nb_steps']))\n",
    "        \n",
    "    def on_train_end(self, logs):\n",
    "        duration = timeit.default_timer() - self.train_start\n",
    "        print('done, took {:.3f} seconds'.format(duration))\n",
    "\n",
    "    def on_episode_begin(self, episode, logs):\n",
    "        self.episode_start[episode] = timeit.default_timer()\n",
    "        self.observations[episode] = []\n",
    "        self.rewards[episode] = []\n",
    "        self.actions[episode] = []\n",
    "        self.metrics[episode] = []\n",
    "        \n",
    "\n",
    "    def on_episode_end(self, episode, logs):\n",
    "        duration = timeit.default_timer() - self.episode_start[episode]\n",
    "        episode_steps = len(self.observations[episode])\n",
    "\n",
    "        # Format all metrics.\n",
    "        metrics = numpy.array(self.metrics[episode])\n",
    "        metrics_template = ''\n",
    "        metrics_variables = []\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('error')\n",
    "            for idx, name in enumerate(self.metrics_names):\n",
    "                if idx > 0:\n",
    "                    metrics_template += ', '\n",
    "                try:\n",
    "                    value = numpy.nanmean(metrics[:, idx])\n",
    "                    metrics_template += '{}: {:f}'\n",
    "                except Warning:\n",
    "                    value = '--'\n",
    "                    metrics_template += '{}: {}'\n",
    "                metrics_variables += [name, value]          \n",
    "        metrics_text = metrics_template.format(*metrics_variables)\n",
    "\n",
    "        nb_step_digits = str(int(numpy.ceil(numpy.log10(self.params['nb_steps']))) + 1)\n",
    "        template = '{step: ' + nb_step_digits + 'd}/{nb_steps}: episode: {episode}, duration: {duration:.3f}s, episode steps: {episode_steps}, steps per second: {sps:.0f}, episode reward: {episode_reward:.3f}, mean reward: {reward_mean:.3f} [{reward_min:.3f}, {reward_max:.3f}], mean action: {action_mean:.3f} [{action_min:.3f}, {action_max:.3f}], {metrics}'\n",
    "        variables = {\n",
    "            'step': self.step,\n",
    "            'nb_steps': self.params['nb_steps'],\n",
    "            'episode': episode + 1,\n",
    "            'duration': duration,\n",
    "            'episode_steps': episode_steps,\n",
    "            'sps': float(episode_steps) / duration,\n",
    "            'episode_reward': numpy.sum(self.rewards[episode]),\n",
    "            'reward_mean': numpy.mean(self.rewards[episode]),\n",
    "            'reward_min': numpy.min(self.rewards[episode]),\n",
    "            'reward_max': numpy.max(self.rewards[episode]),\n",
    "            'action_mean': numpy.mean(self.actions[episode]),\n",
    "            'action_min': numpy.min(self.actions[episode]),\n",
    "            'action_max': numpy.max(self.actions[episode]),\n",
    "            'metrics': metrics_text,\n",
    "        }\n",
    "        \n",
    "        print(template.format(**variables))\n",
    "        '''\n",
    "        Code for saving up weights if the episode reward is higher than the last one\n",
    "        '''\n",
    "        \n",
    "        if numpy.sum(self.rewards[episode])>self.lastreward:\n",
    "            \n",
    "            previousWeights = \"{}/best_weight.hdf5\".format(self.output_path)\n",
    "            if os.path.exists(previousWeights): os.remove(previousWeights)\n",
    "            self.lastreward = numpy.sum(self.rewards[episode])\n",
    "            print(\"The reward is higher than the best one, saving checkpoint weights\")\n",
    "            newWeights = \"{}/best_weight.hdf5\".format(self.output_path)\n",
    "            self.model.save_weights(newWeights, overwrite=True)\n",
    "            \n",
    "        else:\n",
    "            print(\"The reward is lower than the best one, checkpoint weights not updated\")\n",
    "            \n",
    "\n",
    "        # Free up resources.\n",
    "        del self.episode_start[episode]\n",
    "        del self.observations[episode]\n",
    "        del self.rewards[episode]\n",
    "        del self.actions[episode]\n",
    "        del self.metrics[episode]\n",
    "\n",
    "    def on_step_end(self, step, logs):\n",
    "        episode = logs['episode']\n",
    "        self.observations[episode].append(logs['observation'])\n",
    "        self.rewards[episode].append(logs['reward'])\n",
    "        self.actions[episode].append(logs['action'])\n",
    "        self.metrics[episode].append(logs['metrics'])\n",
    "        self.step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym.spaces\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "def calc_profit(action, df, index):\n",
    "    if action == 0:\n",
    "        p = 1\n",
    "    elif action == 1:\n",
    "        p = -1\n",
    "    else:\n",
    "        p = 0\n",
    "    return  p * df[\"c\"][index]\n",
    "\n",
    "def calc_observation(df, index, columns):\n",
    "    return numpy.array([df[col][index] for col in columns])\n",
    "\n",
    "class Game(gym.core.Env):\n",
    "    metadata = {'render.modes': ['human', 'rgb_array']}\n",
    "    def __init__(self, df, columns):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.columns = columns\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        low_bound = numpy.array([0]*len(columns))\n",
    "        high_bound = numpy.array([1]*len(columns))\n",
    "        self.observation_space = gym.spaces.Box(low=low_bound, high=high_bound)\n",
    "        self.time = 0\n",
    "        self.profit = 0\n",
    "        \n",
    "    def step(self, action):\n",
    "        reward = calc_profit(action, self.df, self.time)\n",
    "        self.time += 1\n",
    "        self.profit += reward       \n",
    "        done = self.time == (len(self.df) - 1)\n",
    "        if done:\n",
    "            print(\"profit___{}\".format(self.profit))\n",
    "        info = {}\n",
    "        observation = calc_observation(self.df, self.time, self.columns)\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self.time = 0\n",
    "        self.profit = 0\n",
    "        return calc_observation(self.df, self.time, self.columns)\n",
    "    \n",
    "    def render(self, mode):\n",
    "        pass\n",
    "    \n",
    "    def close(self):\n",
    "        pass\n",
    "    \n",
    "    def seed(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import (\n",
    "    Dense,\n",
    "    Activation,\n",
    "    Flatten,\n",
    "    Input,\n",
    "    concatenate,\n",
    "    Dropout\n",
    ")\n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "\n",
    "    def sample_model(self, obsrvation_shape):\n",
    "        model = Sequential()\n",
    "        model.add(Flatten(input_shape=(1,) + obsrvation_shape))\n",
    "        model.add(Dense(32))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(0.6))\n",
    "        model.add(Dense(16))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(0.6))\n",
    "        model.add(Dense(n_action))\n",
    "        model.add(Activation('linear'))\n",
    "        self.model = model\n",
    "        return model        \n",
    "    \n",
    "    def from_json(self, file_path):\n",
    "        pass\n",
    "    \n",
    "    def to_json(self, output_path):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def agent(model, n_action):\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    policy = EpsGreedyQPolicy(eps=0.1)\n",
    "    dqn_agent = DQNAgent(model=model, nb_actions=n_action,\n",
    "                         memory=memory, nb_steps_warmup=100,\n",
    "                         target_model_update=1e-2, policy=policy)\n",
    "    dqn_agent.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "    return dqn_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "\n",
    "df = pandas.DataFrame({\"a\": numpy.random.rand(1000), \"b\": numpy.random.rand(1000)})\n",
    "df[\"c\"] = df[\"a\"] * df[\"a\"] - df[\"b\"] * df[\"b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback= MyCallback(\"tmp\")\n",
    "columns = [\"a\", \"b\"]\n",
    "env = Game(df, columns)\n",
    "n_action = 2\n",
    "network = Network()\n",
    "model = network.sample_model(env.observation_space.shape)\n",
    "agent_v6 = agent(model, n_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent_v6.fit(env, nb_steps=5000, visualize=False,\n",
    "                  verbose=2, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_v6.load_weights(\"tmp/best_weight.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_v6.forward([0.5, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_v6.backward(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_v6.backward?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
